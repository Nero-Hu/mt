#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
from itertools import izip
import re
import numpy as np
from collections import defaultdict
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    parser.add_argument('-a', '--alpha', default=0.5, type=float,
            help='Number of hypothesis pairs to evaluate')
    parser.add_argument('-g', '--ngram', default=4, type=int,
            help='Ngram used for evaluting bleu')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
    ng = opts.ngram
    feaNum = 16

    featureVec = []
    labelVec = []
    predVec = []

    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]

    #Remove stop words
    stopWordList = []
    puncList=[',','?','!',':',';','-']
    with open("eng-stop-words.txt") as f:
        for pair in f:
            stopWordList = pair.strip().split()

    def removeStopWord(inStr):
        #Use regex to remove punctuation
        return [re.sub(r'[^\w\s]','',removedList) for removedList in inStr if removedList not in stopWordList]

    def collectFeature(h1, h2, ref):
        fVec = [0.0] * feaNum * 2
        #First compute h1 word vec
        gDict = []
        h1Dict = []
        h2Dict = []
        for i in range(ng):
            gDict.append(defaultdict(float))
            h1Dict.append(defaultdict(float))
            h2Dict.append(defaultdict(float))

        for i in range(len(ref)):
            for j in xrange(i+1,i+ng+1):
                if j > len(ref):
                    break
                gDict[j-i-1][tuple(ref[i:j])] += 1.0

        for i in range(len(h1)):
            for j in xrange(i+1,i+ng+1):
                if j > len(h1):
                    break
                if tuple(h1[i:j]) in gDict[j-i-1].keys():
                    h1Dict[j-i-1][tuple(h1[i:j])] += 1.0
                    # modified counts
                    h1Dict[j-i-1][tuple(h1[i:j])] = min(h1Dict[j-i-1][tuple(h1[i:j])], gDict[j-i-1][tuple(h1[i:j])])
                
        
        for i in range(len(h2)):
            for j in xrange(i+1,i+ng+1):
                if j > len(h2):
                    break
                if tuple(h2[i:j]) in gDict[j-i-1].keys():
                    h2Dict[j-i-1][tuple(h2[i:j])] += 1.0
                    # modified counts
                    h2Dict[j-i-1][tuple(h2[i:j])] = min(h2Dict[j-i-1][tuple(h2[i:j])], gDict[j-i-1][tuple(h2[i:j])])

        # get ngram precision, recall, f-measure
        for i in range(ng):
            sc1 = sum(h1Dict[i].values()) +1.0
            sc2 = (sum(h2Dict[i].values()) +1.0)
            #Precision
            fVec[i] = sc1 / max(len(h1)-i+1.0,1.0)
            fVec[i+feaNum] = sc2 / max(len(h1)-i+1.0,1.0)
            #Recall
            fVec[i+4] = sc1 / max(len(ref)-i+1.0,1.0)
            fVec[i+4+feaNum] = sc2 / max(len(ref)-i+1.0,1.0)
            #F measure
            fVec[i+8] = 2.0 * fVec[i] * fVec[i+4] / (fVec[i] + fVec[i+4])
            fVec[i+8+feaNum] = 2.0 * fVec[i+feaNum] * fVec[i+4+feaNum] / (fVec[i+feaNum] + fVec[i+4+feaNum])

        # Get average n-gram precision
        fVec[12] = 0.25 * (fVec[0] + fVec[1] + fVec[2] + fVec[3])
        fVec[12+feaNum] = 0.25 * (fVec[feaNum] + fVec[feaNum+1] + fVec[feaNum+2] + fVec[feaNum+3])

        #Get word count
        fVec[13] = len(h1)
        fVec[13+feaNum] = len(h2)

        #Get function word count
        fc1 = len(removeStopWord(h1))
        fc2 = len(removeStopWord(h2))
        fVec[14] = fc1
        fVec[14+feaNum] = fc2

        #Content count
        fVec[15] = len(h1) - fc1
        fVec[15+feaNum] = len(h2) - fc2

        return fVec
    
	# Collect feature vectors
    lf = open("data/dev.answers")
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        label = lf.readline().strip()
        if label is not None and label != "":
            fv = collectFeature(h1, h2, ref)
            featureVec.append(fv)
            predVec.append(fv)
            labelVec.append(int(label.strip()))
        else:
            predVec.append(collectFeature(h1, h2, ref))

    fV = np.array(featureVec)
    pV = np.array(predVec)
    lV = np.array(labelVec, dtype=int).reshape(-1,1)
    # Add 1 to 1,2,3
    lV += 1
    #Train SVM using scikit-learn
    Z = OneVsRestClassifier(SVC(kernel="rbf")).fit(fV, lV)
    Z = Z.predict(pV)

    for x in range(len(Z)):
        print int(Z[x] -1)

# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
