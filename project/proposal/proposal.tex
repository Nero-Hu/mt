\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{CJKutf8}
\usepackage{listings}
\usepackage[encapsulated]{CJK}
% set the default code style
\lstset{
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=2
}

\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\pagestyle{myheadings}
\markboth{Haitang Hu}{Machine Translation : Project Proposal}


\title{Machine Translation\\Extend Re-ordering on Moses}
\author{Haitang Hu \\
  {\tt hthu@cs.jhu.edu}}
\date{\today}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\section{Overview} % (fold)
\label{sec:Overview}
A hard constrained re-ordering model could potentially hurt our search space, since we simply pruned the possible long distance re-ordering option, which could happen in between language pairs such as Japanese-English because of the different language structures. On the otherhand, an unlimited distortion could cause a complex and large search space, that will degrade the performance of decoding process significantly. So, a wise choice that balanced between search space and performance is crucial. 
% section problem (end)

\section{Methods and Milestones} % (fold)
\label{sec:Methods and Milestones}
Several papers have proposed ideas on dealing with the ``soft'' re-ordering model, this project will survey several approach first, and compare their performance on Moses. Also, another possible way is try to combine these approaches to build up a better weighted model using ensemble learning. 
The general milestones are listed below:
\begin{enumerate}
	\item 5 April, 2015\\
	Finish reading following paper, and decides the must-implement model.
	\begin{itemize}
		\item Dynamically Shaping the Reordering Search Space of Phrase-Based Statistical Machine Translation.\cite{dshape}
		\item Improved Models of Distortion Cost for Statistical Machine Translation.\cite{dc}
		\item Reordering Constraints for Phrase-Based Statistical Machine Translation.\cite{rc}
		\item Advancements in Reordering Models for Statistical Machine Translation.\cite{ar}
		\item Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation.\cite{al}
		\item Inducing Sentence Structure from Parallel Corpora for Reordering.\cite{id}
		\item Source-Side Classifier Preordering for Machine Translation.\cite{ss}
	\end{itemize}
	\item 17 April, 2015\\
	Implementation done.
	\item 21 April, 2015\\
	Interim report.
	\item 26 APril, 2015\\
	System combination research, evaluation.
	\item 1 May, 2015\\
	Final report done.
	\item 8 May, 2015\\
	Presentation.
\end{enumerate}
% section outline (end)

\section{Experimental design} % (fold)
\label{sec:experimental_design}
\subsection{Baseline} % (fold)
\label{sub:baseline}
Baseline system will simply be the default Moses system, using ``hard-constrained'' re-ordering. Hopefully this better re-ordering approach will offer a better performance on decoding. The aimed baseline is to raise the BLEU score of Moses decoding process without affecting performance.
% subsection baseline (end)
\subsection{Evaluation} % (fold)
\label{sub:evaluation}
Since we will relax the constraint of ``hard ''re-ordering, so the developement data set could be potentially more interesting if we both evaluate Japanese-English and French-English pairs. The French-English pair will lie on Europarl\cite{eu}. The Japanese-English pair will lie on the work of NICT which gives a wiki based translation corpus\cite{nict}.
% subsection evaluation (end)
% section experimental_design (end)
\begin{thebibliography}{50}
\bibitem{dshape} Arianna Bisazza and Marcello Federico. \textsl{Dynamically Shaping the Reordering Search Space
of Phrase-Based Statistical Machine Translation}, 2013.

\bibitem{dc} Spence Green, Michel Galley, and Christopher D. Manning. \textsl{Improved Models of Distortion Cost for Statistical Machine Translation}, 2010.

\bibitem{rc} Richard Zens, Hermann Ney, Taro Watanabe and Eiichiro Sumita. \textsl{IReordering Constraints for Phrase-Based Statistical Machine Translation}, 2004.

\bibitem{ar} Minwei Feng and Jan-Thorsten Peter and Hermann Ney. \textsl{Advancements in Reordering Models for Statistical Machine Translation}, 2013.

\bibitem{al} Dmitriy Genze. \textsl{Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation}, 2010.

\bibitem{id} John DeNero and Jakob Uszkoreit. \textsl{Inducing Sentence Structure from Parallel Corpora for Reordering}, 2011.

\bibitem{ss} Uri Lerner and Slav Petrov. \textsl{Source-Side Classifier Preordering for Machine Translation}, 2013.

\bibitem{eu} Philipp Koehn. \textsl{Europarl: A Parallel Corpus for Statistical Machine Translation}, 2005.

\bibitem{nict} National Institute of Information and Communications Technology. \textsl{Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles}, 2012.

\end{thebibliography}
\end{document}
